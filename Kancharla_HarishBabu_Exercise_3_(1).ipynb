{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harishk1998/HarishBabu_INFO5731_Fall2024/blob/main/Kancharla_HarishBabu_Exercise_3_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of Friday, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting **text classification or text mining task** and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features. **Your dataset must be text.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "Sentiment analysis is an interesting task where we aim to categorize text as \"positive,\" \"negative,\" or \"neutral.\"\n",
        "It’s widely used in things like reviews, social media comments, or feedback to gauge how people feel.\n",
        "\n",
        "Here are five types of features that can really help in building a sentiment analysis model:\n",
        "\n",
        "Word Frequencies: The number of times specific words appear in the text can strongly hint at the sentiment.\n",
        "For instance, words like \"great,\" \"awesome,\" or \"fantastic\" often signal a positive sentiment, while \"awful,\" \"poor,\"\n",
        "or \"terrible\" lean towards negative.\n",
        "\n",
        "TF-IDF (Term Frequency-Inverse Document Frequency): This technique helps highlight important words in a document\n",
        "that might be rare across the whole dataset. It gives more weight to unique words in a specific\n",
        "text while reducing the impact of common words like \"the.\"\n",
        "\n",
        "N-grams: These are groups of words that appear together, like \"very nice.\" Instead of looking at single words,\n",
        "we focus on pairs or triples of words (bigrams or trigrams), which give more context and can capture sentiment\n",
        "more effectively.\n",
        "\n",
        "Sentiment Scores: Pre-built tools (like VADER or AFINN) can assign a sentiment score to individual words.\n",
        "Adding up these scores can provide an overall sentiment score for the whole text, making it easier to classify.\n",
        "\n",
        "POS Tags (Part-of-Speech Tags): These tags tell us the grammatical role of each word (like whether it’s a noun, verb,\n",
        "or adjective). Since adjectives and adverbs often carry strong emotions, identifying them can help with sentiment\n",
        "analysis.\n",
        "\n",
        "Each of these features brings something unique to the table. Word frequencies tell us how often certain words pop up,\n",
        "while TF-IDF ensures we focus on important terms. N-grams capture context, sentiment scores offer numerical insight,\n",
        "and POS tags help us understand the structure and emotional weight of the text. Together, these features help paint\n",
        "a full picture of the sentiment behind the words.\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f332faf-7998-4a47-c619-652303cdb755"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Frequencies:\n",
            "   amazing  ambiance  and  anyone  average  bad  but  delicious  dessert  \\\n",
            "0        0         0    1       0        0    0    0          1        0   \n",
            "1        0         0    0       0        0    1    0          0        0   \n",
            "2        1         0    1       1        0    0    0          0        1   \n",
            "3        0         0    0       0        1    0    1          0        0   \n",
            "4        0         1    0       0        0    0    0          0        0   \n",
            "\n",
            "   experience  ...  recommend  restaurant  service  steak  the  this  to  was  \\\n",
            "0           0  ...          0           0        0      0    1     0   0    1   \n",
            "1           1  ...          0           0        0      1    1     0   0    1   \n",
            "2           0  ...          1           0        0      0    1     0   1    1   \n",
            "3           0  ...          0           0        1      0    1     1   0    0   \n",
            "4           0  ...          0           1        0      0    2     0   0    0   \n",
            "\n",
            "   with  would  \n",
            "0     0      0  \n",
            "1     1      0  \n",
            "2     0      1  \n",
            "3     0      0  \n",
            "4     0      0  \n",
            "\n",
            "[5 rows x 34 columns] \n",
            "\n",
            "TF-IDF Scores:\n",
            "    amazing  ambiance       and    anyone   average       bad       but  \\\n",
            "0  0.000000  0.000000  0.329994  0.000000  0.000000  0.000000  0.000000   \n",
            "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.369447  0.000000   \n",
            "2  0.354054  0.000000  0.285648  0.354054  0.000000  0.000000  0.000000   \n",
            "3  0.000000  0.000000  0.000000  0.000000  0.329206  0.000000  0.329206   \n",
            "4  0.000000  0.424127  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "\n",
            "   delicious   dessert  experience  ...  recommend  restaurant   service  \\\n",
            "0   0.409019  0.000000    0.000000  ...   0.000000    0.000000  0.000000   \n",
            "1   0.000000  0.000000    0.369447  ...   0.000000    0.000000  0.000000   \n",
            "2   0.000000  0.354054    0.000000  ...   0.354054    0.000000  0.000000   \n",
            "3   0.000000  0.000000    0.000000  ...   0.000000    0.000000  0.329206   \n",
            "4   0.000000  0.000000    0.000000  ...   0.000000    0.424127  0.000000   \n",
            "\n",
            "      steak       the      this        to       was      with     would  \n",
            "0  0.000000  0.194900  0.000000  0.000000  0.273925  0.000000  0.000000  \n",
            "1  0.369447  0.176043  0.000000  0.000000  0.247423  0.369447  0.000000  \n",
            "2  0.000000  0.168709  0.000000  0.354054  0.237114  0.000000  0.354054  \n",
            "3  0.000000  0.156869  0.329206  0.000000  0.000000  0.000000  0.000000  \n",
            "4  0.000000  0.404198  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "\n",
            "[5 rows x 34 columns] \n",
            "\n",
            "Bigrams:\n",
            "       (the, pasta)  (pasta, was)  (was, delicious)  (delicious, and)  \\\n",
            "count           1.0           1.0               1.0               1.0   \n",
            "count           0.0           0.0               0.0               0.0   \n",
            "count           0.0           0.0               0.0               0.0   \n",
            "count           0.0           0.0               0.0               0.0   \n",
            "count           0.0           0.0               0.0               0.0   \n",
            "\n",
            "       (and, full)  (full, of)  (of, flavor)  (flavor, .)  (i, had)  (had, a)  \\\n",
            "count          1.0         1.0           1.0          1.0       0.0       0.0   \n",
            "count          0.0         0.0           0.0          0.0       1.0       1.0   \n",
            "count          0.0         0.0           0.0          0.0       0.0       0.0   \n",
            "count          0.0         0.0           0.0          0.0       0.0       0.0   \n",
            "count          0.0         0.0           0.0          0.0       0.0       0.0   \n",
            "\n",
            "       ...  (is, great)  (great, .)  (i, really)  (really, loved)  \\\n",
            "count  ...          0.0         0.0          0.0              0.0   \n",
            "count  ...          0.0         0.0          0.0              0.0   \n",
            "count  ...          0.0         0.0          0.0              0.0   \n",
            "count  ...          1.0         1.0          0.0              0.0   \n",
            "count  ...          0.0         0.0          1.0              1.0   \n",
            "\n",
            "       (loved, the)  (the, ambiance)  (ambiance, of)  (of, the)  \\\n",
            "count           0.0              0.0             0.0        0.0   \n",
            "count           0.0              0.0             0.0        0.0   \n",
            "count           0.0              0.0             0.0        0.0   \n",
            "count           0.0              0.0             0.0        0.0   \n",
            "count           1.0              1.0             1.0        1.0   \n",
            "\n",
            "       (the, restaurant)  (restaurant, .)  \n",
            "count                0.0              0.0  \n",
            "count                0.0              0.0  \n",
            "count                0.0              0.0  \n",
            "count                0.0              0.0  \n",
            "count                1.0              1.0  \n",
            "\n",
            "[5 rows x 51 columns] \n",
            "\n",
            "Sentiment Scores:\n",
            "     neg    neu    pos  compound\n",
            "0  0.000  0.654  0.346    0.5719\n",
            "1  0.304  0.696  0.000   -0.5423\n",
            "2  0.000  0.548  0.452    0.7644\n",
            "3  0.000  0.614  0.386    0.7684\n",
            "4  0.000  0.589  0.411    0.6361 \n",
            "\n",
            "POS Tags:\n",
            "   The pasta  was delicious  and full   of flavor    .    I  ... average  \\\n",
            "0   DT    NN  VBD        JJ   CC   JJ   IN     NN    .  NaN  ...     NaN   \n",
            "1  NaN   NaN  VBD       NaN  NaN  NaN  NaN    NaN    .  PRP  ...     NaN   \n",
            "2   DT   NaN  VBD       NaN   CC  NaN  NaN    NaN  NaN  PRP  ...     NaN   \n",
            "3  NaN   NaN  NaN       NaN  NaN  NaN  NaN    NaN    .  NaN  ...      JJ   \n",
            "4  NaN   NaN  NaN       NaN  NaN  NaN   IN    NaN    .  PRP  ...     NaN   \n",
            "\n",
            "  service  but food   is great really loved ambiance restaurant  \n",
            "0     NaN  NaN  NaN  NaN   NaN    NaN   NaN      NaN        NaN  \n",
            "1     NaN  NaN  NaN  NaN   NaN    NaN   NaN      NaN        NaN  \n",
            "2     NaN  NaN  NaN  NaN   NaN    NaN   NaN      NaN        NaN  \n",
            "3      NN   CC   NN  VBZ    JJ    NaN   NaN      NaN        NaN  \n",
            "4     NaN  NaN  NaN  NaN   NaN     RB   VBD       NN         NN  \n",
            "\n",
            "[5 rows x 41 columns] \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk scikit-learn pandas\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk import pos_tag\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "\n",
        "sample_texts = [\n",
        "    \"The pasta was delicious and full of flavor.\",\n",
        "    \"I had a bad experience with the steak; it was overcooked.\",\n",
        "    \"The dessert was amazing, and I would recommend it to anyone!\",\n",
        "    \"This place has average service, but the food is great.\",\n",
        "    \"I really loved the ambiance of the restaurant.\"\n",
        "]\n",
        "\n",
        "count_vectorizer = CountVectorizer()\n",
        "word_count_matrix = count_vectorizer.fit_transform(sample_texts)  # Count words\n",
        "word_count_df = pd.DataFrame(word_count_matrix.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
        "\n",
        "#Calculate TF-IDF scores\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(sample_texts)  # Calculate TF-IDF\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())  # Create a DataFrame\n",
        "#Create bigrams (pairs of words)\n",
        "bigrams = [list(ngrams(word_tokenize(text.lower()), 2)) for text in sample_texts]\n",
        "bigram_counts = [pd.Series(bigram).value_counts() for bigram in bigrams]  # Count bigrams\n",
        "bigram_df = pd.DataFrame(bigram_counts).fillna(0)\n",
        "# Get sentiment scores\n",
        "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
        "sentiment_results = [sentiment_analyzer.polarity_scores(text) for text in sample_texts]  # Get scores\n",
        "sentiment_df = pd.DataFrame(sentiment_results)\n",
        "#Get part-of-speech tags\n",
        "pos_tags_list = [pos_tag(word_tokenize(text)) for text in sample_texts]  # Get POS tags\n",
        "pos_df = pd.DataFrame([{word: tag for word, tag in tags} for tags in pos_tags_list])  # Create a DataFrame\n",
        "\n",
        "\n",
        "print(\"Word Frequencies:\")\n",
        "print(word_count_df, \"\\n\")\n",
        "\n",
        "print(\"TF-IDF Scores:\")\n",
        "print(tfidf_df, \"\\n\")\n",
        "\n",
        "print(\"Bigrams:\")\n",
        "print(bigram_df, \"\\n\")\n",
        "\n",
        "print(\"Sentiment Scores:\")\n",
        "print(sentiment_df, \"\\n\")\n",
        "\n",
        "print(\"POS Tags:\")\n",
        "print(pos_df, \"\\n\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b37a748-d827-4db6-8ba8-399bcbe3f0a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked Features based on Chi-Squared Score:\n",
            "       Feature  Chi2 Score\n",
            "0   absolutely    0.644494\n",
            "1    wonderful    0.644494\n",
            "2           it    0.644494\n",
            "3        loved    0.644494\n",
            "4         will    0.612372\n",
            "5   definitely    0.612372\n",
            "6    delicious    0.612372\n",
            "7      dessert    0.612372\n",
            "8         what    0.612372\n",
            "9       return    0.612372\n",
            "10     nothing    0.305377\n",
            "11     average    0.305377\n",
            "12     special    0.305377\n",
            "13     service    0.305377\n",
            "14          ve    0.246451\n",
            "15        this    0.246451\n",
            "16        meal    0.246451\n",
            "17          is    0.246451\n",
            "18         had    0.246451\n",
            "19        ever    0.246451\n",
            "20       worst    0.246451\n",
            "21        nice    0.240023\n",
            "22    ambiance    0.240023\n",
            "23         but    0.240023\n",
            "24         bad    0.240023\n",
            "25         the    0.154982\n",
            "26         was    0.079079\n",
            "27        food    0.055113\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import chi2\n",
        "import numpy as np\n",
        "\n",
        "# Download necessary NLTK data files\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Sample text data and their sentiment labels\n",
        "texts = [\n",
        "    \"The food was absolutely wonderful! I loved it.\",\n",
        "    \"This is the worst meal I've ever had.\",\n",
        "    \"The service was average, nothing special.\",\n",
        "    \"What a delicious dessert! I will definitely return.\",\n",
        "    \"The ambiance was nice, but the food was bad.\"\n",
        "]\n",
        "labels = [1, 0, 0, 1, 0]  # 1 for positive, 0 for negative\n",
        "\n",
        "# Convert text data into numeric format using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X = tfidf_vectorizer.fit_transform(texts)\n",
        "\n",
        "# Perform Chi-Squared feature selection\n",
        "chi2_values, p_values = chi2(X, labels)\n",
        "\n",
        "# Create a DataFrame for features and their Chi-Squared scores\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "chi2_df = pd.DataFrame({'Feature': feature_names, 'Chi2 Score': chi2_values})\n",
        "\n",
        "# Rank features by Chi-Squared score in descending order\n",
        "chi2_df = chi2_df.sort_values(by='Chi2 Score', ascending=False)\n",
        "\n",
        "# Display the ranked features\n",
        "print(\"Ranked Features based on Chi-Squared Score:\")\n",
        "print(chi2_df.reset_index(drop=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9fcb64c-400a-4bbb-f4c3-bfbf177a98e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked Documents based on Similarity to Query:\n",
            "                                                Text  Similarity\n",
            "0     The food was absolutely wonderful! I loved it.    0.846537\n",
            "1  What a delicious dessert! I will definitely re...    0.738802\n",
            "2              This is the worst meal I've ever had.    0.629565\n",
            "3       The ambiance was nice, but the food was bad.    0.613991\n",
            "4          The service was average, nothing special.    0.586754\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "!pip install transformers torch scikit-learn\n",
        "\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Sample text data (documents)\n",
        "texts = [\n",
        "    \"The food was absolutely wonderful! I loved it.\",\n",
        "    \"This is the worst meal I've ever had.\",\n",
        "    \"The service was average, nothing special.\",\n",
        "    \"What a delicious dessert! I will definitely return.\",\n",
        "    \"The ambiance was nice, but the food was bad.\"\n",
        "]\n",
        "\n",
        "# Load BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to encode texts using BERT\n",
        "def encode_texts(texts):\n",
        "    # Tokenize and convert texts to tensor format\n",
        "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)  # Get BERT outputs\n",
        "    return outputs.last_hidden_state.mean(dim=1)  # Average pooling over token embeddings\n",
        "\n",
        "# Encode the documents\n",
        "document_embeddings = encode_texts(texts)\n",
        "\n",
        "# Define and encode a query\n",
        "query = \"I had a fantastic dining experience!\"\n",
        "query_embedding = encode_texts([query])  # Encode the query\n",
        "\n",
        "# Calculate cosine similarity between query and documents\n",
        "similarities = cosine_similarity(query_embedding, document_embeddings).flatten()\n",
        "\n",
        "# Create a DataFrame for texts and similarity scores\n",
        "results_df = pd.DataFrame({\n",
        "    'Text': texts,\n",
        "    'Similarity': similarities\n",
        "})\n",
        "\n",
        "# Rank documents by similarity in descending order\n",
        "results_df = results_df.sort_values(by='Similarity', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Display the ranked results\n",
        "print(\"Ranked Documents based on Similarity to Query:\")\n",
        "print(results_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "\n",
        "Working on extracting features from text data was an enlightening journey. I learned a lot about key techniques\n",
        "in Natural Language Processing (NLP)\n",
        "\n",
        "TF-IDF Vectorization: Understanding how to quantify word importance based on frequency across documents was\n",
        "eye-opening and foundational for text analysis.\n",
        "\n",
        "Feature Selection: Using Chi-Squared for identifying important features showed me how to filter out less significant\n",
        "words, ultimately improving model performance.\n",
        "\n",
        "BERT Embeddings: Learning to use BERT for encoding text highlighted the power of deep learning in understanding context,\n",
        "which is vital for tasks like semantic similarity.\n",
        "\n",
        "\n",
        "I have faced a few challenges along the way:\n",
        "BERT Outputs: Figuring out how to extract and average embeddings from BERT was a bit confusing at first, requiring some\n",
        "extra reading and understanding of the model’s structure.\n",
        "\n",
        "Cosine Similarity: Implementing cosine similarity to rank documents was tricky. I had to be careful with input formatting\n",
        "and output interpretation.\n",
        "\n",
        "Library Issues: Managing library installations and ensuring compatibility among transformers and torch was sometimes\n",
        "frustrating.\n",
        "\n",
        "Relevance to Your Field of Study\n",
        "This exercise is super relevant to NLP. Feature extraction and selection are crucial steps in preparing text data for\n",
        "various tasks, like sentiment analysis and topic modeling. Knowing how to effectively represent text helps in creating\n",
        "more accurate models.\n",
        "\n",
        "With deep learning becoming a big part of NLP, getting familiar with embedding techniques like BERT is essential for\n",
        "tackling more complex applications in the future. Overall, this exercise has been both rewarding and informative,\n",
        "setting a solid foundation for my exploration of NLP.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}