{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harishk1998/HarishBabu_INFO5731_Fall2024/blob/main/Kancharla_Harishbabu_Exercise_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n",
        "Question:\n",
        "What are the most common themes or categories of books available on an online bookstore,\n",
        "and how does the distribution of these categories vary across the website?\n",
        "\n",
        "Data to Collect:\n",
        "1. Book titles\n",
        "2. Book categories\n",
        "3. Price and availability of each book\n",
        "\n",
        "Steps for Collecting Data:\n",
        "Web Scraping Setup:\n",
        "We have to use Python’s requests and BeautifulSoup libraries to scrape the website books.toscrape.com,\n",
        "which contains a collection of books.\n",
        "\n",
        "Data Collection:\n",
        "Scrape the book titles, categories, prices, and availability from multiple pages of the website.\n",
        "Collect at least 1000 book entries to ensure enough data for analysis.\n",
        "\n",
        "\n",
        "The fainal step is to save the scraped data in a CSV file for easy analysis of book categories, price distribution, and availability.\n"
      ],
      "metadata": {
        "id": "cikVKDXdTbzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "url = 'http://books.toscrape.com/catalogue/page-{}.html'\n",
        "with open('books_data.csv', mode='w', newline='', encoding='utf-8-sig') as file:\n",
        "    a = csv.writer(file)\n",
        "    a.writerow(['Title', 'Price', 'Availability'])\n",
        "    for i in range(1, 11):\n",
        "        res = requests.get(url.format(i))\n",
        "        soup = BeautifulSoup(res.content, 'html.parser')\n",
        "        books = soup.find_all('article', class_='product_pod')\n",
        "        for j in books:\n",
        "            title = j.find('h3').find('a')['title']\n",
        "            price = j.find('p', class_='price_color').text\n",
        "            availability = j.find('p', class_='instock availability').text.strip()\n",
        "            a.writerow([title, price, availability])\n",
        "        print(f\"Scraped page {i}\")\n",
        "\n",
        "print(\"Data collection completed and saved to 'books_data.csv'.\")\n",
        "#After executing the code the books_data.csv file was created in the folder\n"
      ],
      "metadata": {
        "id": "4XvRknixTh1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83e1ec12-f284-468e-ef8b-a2e5bb5d38e2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped page 1\n",
            "Scraped page 2\n",
            "Scraped page 3\n",
            "Scraped page 4\n",
            "Scraped page 5\n",
            "Scraped page 6\n",
            "Scraped page 7\n",
            "Scraped page 8\n",
            "Scraped page 9\n",
            "Scraped page 10\n",
            "Data collection completed and saved to 'books_data.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YaGLbSHHB8Ej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edf6ef15-f461-49ea-a2df-f4c4c502de67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 100 articles so far.\n",
            "Collected 200 articles so far.\n",
            "Collected 300 articles so far.\n",
            "Collected 400 articles so far.\n",
            "Collected 500 articles so far.\n",
            "Collected 600 articles so far.\n",
            "Collected 700 articles so far.\n",
            "Collected 800 articles so far.\n",
            "Collected 900 articles so far.\n",
            "Collected 1000 articles so far.\n",
            "Finished collecting 1000 articles. Data saved to articles.csv.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "url = 'https://api.crossref.org/works'\n",
        "file = 'articles.csv'\n",
        "data = []\n",
        "total = 0\n",
        "limit = 100\n",
        "start = 0\n",
        "keyword = 'XYZ'\n",
        "start_year = 2014\n",
        "end_year = 2024\n",
        "def wait(attempts):\n",
        "    wait_time = min(2 ** attempts, 60)\n",
        "    print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
        "    time.sleep(wait_time)\n",
        "\n",
        "while total < 1000:\n",
        "    retries = 0\n",
        "    while True:\n",
        "        params = {\n",
        "            'query': keyword,\n",
        "            'offset': start,\n",
        "            'rows': limit,\n",
        "            'filter': f'from-pub-date:{start_year}-01-01,until-pub-date:{end_year}-12-31'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url, params=params)\n",
        "            if response.status_code == 429:\n",
        "                retries += 1\n",
        "                wait(retries)\n",
        "                continue\n",
        "            response.raise_for_status()\n",
        "            json_data = response.json()\n",
        "            for item in json_data.get('message', {}).get('items', []):\n",
        "                year = item.get('created', {}).get('date-parts', [[None]])[0][0]\n",
        "                if year and start_year <= int(year) <= end_year:\n",
        "                    title = item.get('title', ['No title'])[0]\n",
        "                    venue = item.get('container-title', ['No venue'])[0]\n",
        "                    authors = ', '.join(author.get('name', 'No author') for author in item.get('author', []))\n",
        "                    abstract = item.get('abstract', 'No abstract')\n",
        "                    data.append([title, venue, year, authors, abstract])\n",
        "                    total += 1\n",
        "                    if total >= 1000:\n",
        "                        break\n",
        "            start += limit\n",
        "            print(f\"Collected {total} articles so far.\")\n",
        "            break\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            break\n",
        "df = pd.DataFrame(data, columns=['Title', 'Venue', 'Year', 'Authors', 'Abstract'])\n",
        "df.to_csv(file, index=False)\n",
        "print(f\"Finished collecting {total} articles. Data saved to {file}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MtKskTzbCLaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8874411-0168-43e3-90a0-6510f629abd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting sgmllib3k (from feedparser)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=b91557a4595ac0f7c89b939d7c4505fd727016c8698b5e8813173d8b179fe722\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser\n",
            "Successfully installed feedparser-6.0.11 sgmllib3k-1.0.0\n",
            "Data has been saved to 'reddit_data.csv'\n"
          ]
        }
      ],
      "source": [
        "!pip install feedparser\n",
        "import feedparser\n",
        "import pandas as pd\n",
        "def scrape_reddit(subreddit):\n",
        "    url = f'https://www.reddit.com/r/{subreddit}/top/.rss'\n",
        "    feed = feedparser.parse(url)\n",
        "    data = []\n",
        "    for entry in feed.entries:\n",
        "        title = entry.title\n",
        "        summary = entry.summary\n",
        "        link = entry.link\n",
        "        published = entry.published\n",
        "        author = entry.author\n",
        "        data.append([title, summary, link, published, author])\n",
        "    df = pd.DataFrame(data, columns=['Title', 'Summary', 'Link', 'Published', 'Author'])\n",
        "    return df\n",
        "subreddit = 'Python'\n",
        "df = scrape_reddit(subreddit)\n",
        "df.to_csv('reddit_data.csv', index=False)\n",
        "print(\"Data has been saved to 'reddit_data.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "I learnt a lot overall from working on the web scraping assignments. My knowledge of the structure of data on\n",
        "web sites and how to extract it using programs like BeautifulSoup and Selenium has significantly improved.\n",
        "Understanding the differences between static and dynamic content as well as how APIs may be helpful for data\n",
        "scraping from websites like Reddit and Twitter was interesting to me. Finding a way to obtain the information\n",
        "I needed from several sources felt nice.\n",
        "\n",
        "I did run into some difficulties, particularly on sites like Twitter where obtaining data without an\n",
        "API key was restricted. Managing SSL certificate issues while scraping was another difficult aspect,\n",
        "but I was able to resolve it by adjusting a few parameters. It was a little challenging at first to use\n",
        "Selenium to handle JavaScript for websites with dynamic content (like Instagram), but I ultimately figured it out.\n",
        "\n",
        "The ability to gather information from the internet is quite beneficial for numerous purposes.\n",
        "In my line of work, real-time data from web sources can be very helpful for trend analysis,\n",
        "machine learning data collection, or just conducting targeted study. Making better decisions or developing\n",
        " more robust data-driven projects can both benefit from it.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "akAVJn9YBTQT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FBKvD6O_TY6e",
        "E9RqrlwdTfvl",
        "03jb4GZsBkBS",
        "jJDe71iLB616",
        "55W9AMdXCSpV",
        "4ulBZ6yhCi9F",
        "6SmvS7nSfbj8",
        "sZOhks1dXWEe"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}